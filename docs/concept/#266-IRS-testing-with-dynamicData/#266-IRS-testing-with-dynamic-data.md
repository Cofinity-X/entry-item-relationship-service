# Overview

# Brief summary

Previously tests were based on static data which were each one testdata file for asPlanned and asBuilt generated by TDG-team. Based on this files data provider registered their data.

In the future this approach will change and "dynamic data" will be available for the project.

For conceptional work it is necessary to know what "dynamic data" means:
~~What is known and where to get the globalAssetId for IRS requests?~~ → globalAssetIds are given!
~~How to get information about the parts for specific globalAssetIds for test preparations?~~ → not possible to get them, tests have to be less concrete in expected data.
Who will provide configured testdata for e.g. ESS or other desired use cases?
~~How the data will be published and what will be published in concrete?~~ → not necessary to know since the globalAssetIds are given.

# Problem Statement

"Dynamic data" is not based on any testdataset and will be provided by data provider continuously. As data consumer there is no information given about the data constellation in advance.

Following topics have to be testable with dynamic data as it was possible with static data:
* Use cases for **ESS** with expected and known **incident BPNS**. It is necessary to have known configured BPNS for desired expected test results.
* Use cases for **hops count** and **impacted supplier on first tier level** which has do be known to match the expected test results.
* Use cases for **submodels** for desired asset which have to be tested by expected data which has to be known.
* Use cases to check **relationships** for desired asset.
* Use cases for correct **tombstones** if e.g. the requested aspect is not available or for ESS validations.

# Assumption
GlobalAssetId and BPN which can be used for tests are known.


# Concept

## How to deal with "dynamic testdata" for IRS tests?
Since only testable globalAssetIds with corresponding BPN is known the following details are unknown and have to be considered for the test design:
* submodel aspects
* lifecycle
* direction
* depth

As test preparation first it is necessary to find out if the globalAssetId is existing and what is within the shell.
Therefore the DTR has to be requested with given globalAssetId to get the AAS-identifier. (https://irs-aas-registry.dev.demo.catena-x.net/semantics/registry/api/v3.0/lookup/shells?assetIds=[{"name":"globalAssetId","value":"id"}])
After decoding the responsed AAS-identifier to base64 it is now possible to request the shell. (https://irs-aas-registry.dev.demo.catena-x.net/semantics/registry/api/v3.0/shell-descriptors/dXJuOnV1aWQ6Nzg4ZGMxYmYtNmM5Yy00ZTliLWE0ODktN2UzY2Y5MzAyNzRi)

```mermaid
graph LR
    A[globalAssetId] --> B{Request AAS-registry \n for shells}
    B -- id registered --> C[get AAS-identifier]
    B -- id not registered --> X{start IRS-job without \n submodel aspects} --> XX(expected: \n tombstone 'id not registered')
    C -- AAS-identifier existing --> D[decode AAS-identifier to Base64]
    D --> E{Request shell-descriptors \n in AAS-registry}
    E --> F[get available submodel aspects]
```


### What submodel aspects can be requested?
**Submodels** which can be requested for desired globalAssetId can be found in the **AAS registry**. The response contain a list of submodel descriptors.
Once the available submodels are known the request job for the test has to be adjusted with them. The **expectations** are following:
* At least one submodel for each requested aspectType has to exist in the submodels array of job response.
* If requested submodel is not existing → **tombstone** with failed SubmodelRequest have to exist.
* **Relationships** have to exist.
* If _lookupBPNs_ = _true_ → **bpns** have to contain at least one object.
* To observe the performance topic the expected duration of jobs complete should be set dependent on the amount of requested aspects and the depth.

```mermaid
graph LR
    A[globalAssetId + BPNL] --> B{Request AAS-registry}
    B -- submodelDescriptors available --> C[get available submodels]
    B -- id not registered --> D{start IRS-job without \n submodel aspects} --> DD(expected: \n tombstone 'id not registered')
    C --> E{start IRS-job \n with submodel aspects}
    E --> F(expected: \n - 1-n submodels for each requested aspectType \n - 1-n relationships \n - bpns)
```

### Which lifecycle does the globalAssetId belongs to?
Lifecycle is decided based on submodel-aspects: (?)
* If SerialPart existing -> asBuilt lifecycle
* If PartAsPlanned -> asPlanned lifecycle


### Which direction have to be requested, upward or downward?
Direction is decided based on the given submodel-aspects:
* SingleLevelBomAsBuilt -> asBuilt + downward 
* SingleLevelBomAsPlanned -> asPlanned + downward
* SingleLevelBomAsSpecified -> asSpecified + downward
* SingleLevelUsageAsBuilt -> asBuilt + upward

So the direction test parameters will be set like described.


### What depth can be expected?
Currently there is no effective way to find out the depth of desired globalAssetId. Decided could be the following:
* IRS tests are requested with depth = 10
* Performance tests will be run with depth >50


## Testing tombstones with negative scenarios. TBD
Tombstones can be tested based available informations. Following scenarios 
* globalAssetId not known...
* request with not available submodels...
* failed asyncFetchedItems = amount of tombstones

```mermaid
graph LR
    A[globalAssetId + BPNL] --> B{Request AAS-registry}
    B -- id not registered --> D{start IRS-job without \n submodel aspects} --> DD(expected: \n tombstone 'id not registered')
```

Hints:
* Concept for Tests scenario implementation requests for all AAS shells for a preconfigured dtr to be tested. (DTR are known in advance ) E2E test scenario could use multiple DTR registry to be tested.
* Concept for performance and duration issues the amount of AAS data used for testing could be configured for a specific environment (DEV, INT, STABLE, etc)
* Each AAS shell and the corresponding IRSJob is tested against a set of test scenarios (ESS, IRS up, IRS down) which contains a predefined set of constraints and conditions which ensures the quality and the correctness of IRS job and IRS JobResponses without knowing the concrete structure of the job.


## How to test ESS features? TBD
ESS is currently tested with tavern tests which are based on a specific testdataset. This contains all necessary data for several usecases to test.

Those usecases include following topics:
* check for _supplyChainImpacted_ = Yes/No/Unknown
* check for _supplyChainhops_ which have to match the exact number of hops
* check for _supplyChainFirstLevelBpn_

It has to be investigated if those tests can be done with dynamic testdata at all. TBD!

Hints:
* For special use cases ESS where we expect an incident we use a pre known BPNS which is configurable within the test scenario

## How to test DIL features? WIP
Currently the whole DIL features are not merged to main and  

## How to run performance tests with "dynamic testdata"? WIP
TBD: take from part one with bigger depth and check runtime, amount of finished "asyncFetchedItems = completed".


Hints:
* Basis set of constraints are defined and implemented in cucumber tests and could be extended later on.

# LOP

# Decision
